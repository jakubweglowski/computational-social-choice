{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6097dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, os, sys\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyamg\n",
    "import random as rnd\n",
    "import scipy as scp\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import pylab as py\n",
    "import pingouin as pg\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN, MeanShift, SpectralClustering\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18a8a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nie wywoływać bez istotnego powodu!\n",
    "wordVectors = {}\n",
    "print(\"Loading word vectors...this will take a while\")\n",
    "\n",
    "start = time.time()\n",
    "print('Początek obliczeń o godzinie', time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "\n",
    "checkpoint = time.time()\n",
    "\n",
    "filename = 'C:/Users/priva/OneDrive/Desktop/STUDIA/Proseminarium MIM/cc.pl.300.vec/cc.pl.300.vec'\n",
    "f = open(filename, \"r\", encoding='utf-8')\n",
    "lines = f.readlines()\n",
    "for i,line in enumerate(lines):\n",
    "    if i % 100000 == 0:\n",
    "        print(i,\"/ 2000000. Od poprzedniego checkpointu minęło\", round((time.time() - checkpoint)/60, 2), \"minuty.\")\n",
    "        checkpoint = time.time()\n",
    "    line = line\n",
    "    token = line.split(' ')\n",
    "    wordVectors[token[0]] = token[1:]\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('Słowa wgrywały się', round((time.time() - start)/60, 2), 'minut.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument sentences musi być listą, której elementami są zdania, których podobieństwo chcemy ocenić\n",
    "def sentences_similarity(sentences: list, print_warning = False, print_description = False, path = ''):\n",
    "    \n",
    "    # liczba wgranych zdań\n",
    "    N = len(sentences)\n",
    "    \n",
    "    if N < 2:\n",
    "        stats = {}\n",
    "        stats['nb of sentences'] = N\n",
    "        stats['mean'] = 0.5\n",
    "        stats['std dev'] = 0\n",
    "        stats['min'] = 0.5\n",
    "        stats['max'] = 0.5\n",
    "\n",
    "        return stats\n",
    "    \n",
    "    # lista na podstawowe statystyki\n",
    "    # jeśli tu jesteśmy, to znaczy że dostaliśmy przynajmniej dwa zdania do oceny\n",
    "    stat_values = []\n",
    "    \n",
    "    if path != '':   \n",
    "        with open(path, 'r', newline='', encoding=\"utf-8\") as csvfile:\n",
    "            projects = {}\n",
    "            section = \"\"\n",
    "            reader = csv.reader(csvfile, delimiter=';')\n",
    "            for row in reader:\n",
    "                if str(row[0]).strip().lower() in [\"meta\", \"projects\", \"votes\"]:\n",
    "                    section = str(row[0]).strip().lower()\n",
    "                    header = next(reader)\n",
    "                elif section == \"projects\":\n",
    "                    projects[row[0]] = {}\n",
    "                    for it, key in enumerate(header[1:]):\n",
    "                        projects[row[0]][key.strip()] = row[it+1].strip()\n",
    "                        \n",
    "        Dist_Array_ling = pd.read_csv(path.replace('.pb', '.csv'))\n",
    "        Dist_Array_ling.index = Dist_Array_ling.columns  \n",
    "        \n",
    "        for s1 in sentences:\n",
    "            for s2 in sentences:\n",
    "                if s1 == s2:\n",
    "                    continue\n",
    "                else:\n",
    "                    for key1 in list(projects.keys()):\n",
    "                        if projects[key1]['name'] == s1:\n",
    "                            break\n",
    "                    for key2 in list(projects.keys()):\n",
    "                        if projects[key2]['name'] == s2:\n",
    "                            break\n",
    "                    # teraz key1 orz key2 to numery badanych projektów\n",
    "                    \n",
    "                    if str(np.asmatrix(Dist_Array_ling.loc[[key1], [key2]])[0, 0]) == 'nan':\n",
    "                        continue\n",
    "                    \n",
    "                    if key1 < key2:\n",
    "                        #print(f\"Oceniamy zdania '{s1}' oraz '{s2}'. Podobieństwo: {1-np.asmatrix(Dist_Array_ling.loc[[key1], [key2]])[0, 0]}\")\n",
    "                        #print(f\"Odpowiadają im klucze '{key1}' oraz '{key2}'.\")\n",
    "                        stat_values.append(1-np.asmatrix(Dist_Array_ling.loc[[key1], [key2]])[0, 0])\n",
    "                        #print('Wartość zapisano.')\n",
    "                                \n",
    "    \n",
    "    elif path == '':\n",
    "\n",
    "        # Krok 1 -- ze zdań robimy słowa\n",
    "        words_by_sentence, words = {}, set()\n",
    "        for i,sentence in enumerate(sentences):\n",
    "            words_by_sentence[i] = sentence.rstrip().split(' ')\n",
    "            words_by_sentence[i] = [w.strip('.,!?\"') for w in list(words_by_sentence[i])]\n",
    "            words_by_sentence[i] = list(filter(lambda k: len(k) > 2, words_by_sentence[i]))\n",
    "            words.update(words_by_sentence[i])\n",
    "\n",
    "        # Krok 2 -- obliczamy wektory zdań\n",
    "        wordVectorLength, zeroVectorCount = 300, 0\n",
    "        docVectors = np.zeros( (N, wordVectorLength), dtype='float32')\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            tokens = wordVectors.get(word)\n",
    "            wv = np.asarray(tokens, dtype='float32')\n",
    "            if word in wordVectors:\n",
    "                for i in range(N):\n",
    "                    if word in words_by_sentence[i]:\n",
    "                        docVectors[i] = docVectors[i] + wv/np.linalg.norm(wv)\n",
    "            else:\n",
    "                zeroVectorCount = zeroVectorCount + 1\n",
    "\n",
    "        if print_warning == True:\n",
    "            print ('# words not found in fasttext..', zeroVectorCount)\n",
    "\n",
    "        for i in range(N):\n",
    "            docVectors[i] = docVectors[i]/np.linalg.norm(docVectors[i])\n",
    "\n",
    "        # Krok 3 -- zapisujemy statystyki\n",
    "        for i in range(N):\n",
    "            for j in range(i):\n",
    "                \n",
    "                if print_description == True:\n",
    "                    \n",
    "                    print('Cosine Similarity:\\n', \n",
    "                          sentences[i], '\\n&\\n', sentences[j], ':', \n",
    "                          np.dot(docVectors[i], docVectors[j]), '\\n')\n",
    "                    \n",
    "                stat_values.append(np.dot(docVectors[i], docVectors[j]))\n",
    "\n",
    "                \n",
    "    # Krok 4 -- podsumowanie częstościowe\n",
    "\n",
    "    # wartości które zapisujemy, to:\n",
    "    # 1. liczba zdań\n",
    "    # 2. średnie podobieństwo pomiędzy parami zdań\n",
    "    # 3. odchylenie standardowe tego podobieństwa\n",
    "    # 4. najmniejszą i największą wartość podobieństwa\n",
    "\n",
    "    stats = {}\n",
    "    stats['nb of sentences'] = N\n",
    "    stats['mean'] = np.mean(stat_values)\n",
    "    stats['std dev'] = np.std(stat_values)\n",
    "    stats['min'] = min(stat_values)\n",
    "    stats['max'] = max(stat_values)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af202955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument clusters musi być słownikiem, którego elementami są listy projektów w poszczególnych klastrach\n",
    "def assess_clustering(clusters: dict, stat = 'mean', path = ''):\n",
    "    \n",
    "    #print(path)\n",
    "    \n",
    "    score = []\n",
    "    weights = []\n",
    "    \n",
    "    # wagi do średniej ważonej\n",
    "    for c in list(clusters.values()):\n",
    "        weights.append(sentences_similarity(c, path = path)['nb of sentences'])\n",
    "        \n",
    "    if stat == 'mean':\n",
    "        for c in list(clusters.values()):\n",
    "            score.append(sentences_similarity(c, path = path)['mean'])\n",
    "        final_score = np.average(score, weights = weights)\n",
    "        return final_score\n",
    "    \n",
    "    elif stat == 'std dev':\n",
    "        for c in list(clusters.values()):\n",
    "            score.append(sentences_similarity(c, path = path)['std dev'])\n",
    "        final_score = np.average(score, weights = weights)\n",
    "        return final_score\n",
    "    \n",
    "    elif stat == 'max':\n",
    "        for c in list(clusters.values()):\n",
    "            score.append(sentences_similarity(c, path = path)['max'])\n",
    "        final_score = np.average(score, weights = weights)\n",
    "        return final_score\n",
    "    \n",
    "    elif stat == 'min':\n",
    "        for c in list(clusters.values()):\n",
    "            score.append(sentences_similarity(c, path = path)['min'])\n",
    "        final_score = np.average(score, weights = weights)\n",
    "        return final_score\n",
    "    \n",
    "    elif stat == 'numbers':\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c23cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def projects_similarity(clusters: dict, path):\n",
    "    \n",
    "    # otwieramy plik i odczytujemy dane\n",
    "    with open(path, 'r', newline='', encoding=\"utf-8\") as csvfile:\n",
    "        meta = {}\n",
    "        projects = {}\n",
    "        votes = {}\n",
    "        section = \"\"\n",
    "        header = []\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        for row in reader:\n",
    "            if str(row[0]).strip().lower() in [\"meta\", \"projects\", \"votes\"]:\n",
    "                section = str(row[0]).strip().lower()\n",
    "                header = next(reader)\n",
    "            elif section == \"meta\":\n",
    "                meta[row[0]] = row[1].strip()\n",
    "            elif section == \"projects\":\n",
    "                projects[row[0]] = {}\n",
    "                for it, key in enumerate(header[1:]):\n",
    "                    projects[row[0]][key.strip()] = row[it+1].strip()\n",
    "            elif section == \"votes\":\n",
    "                votes[row[0]] = {}\n",
    "                for it, key in enumerate(header[1:]):\n",
    "                    votes[row[0]][key.strip()] = row[it+1].strip()\n",
    "    \n",
    "    df = Clustering_from_file(path, variant = 'dataframe')[0]\n",
    "    ans = []\n",
    "    \n",
    "    for c in list(clusters.values()):\n",
    "        \n",
    "        for proj1 in c:\n",
    "            for proj2 in c:\n",
    "                \n",
    "                if proj1 == proj2:\n",
    "                    continue\n",
    "                \n",
    "                for key1 in list(projects.keys()):\n",
    "                    if projects[key1]['name'] == proj1:\n",
    "                        break\n",
    "                for key2 in list(projects.keys()):\n",
    "                    if projects[key2]['name'] == proj2:\n",
    "                        break\n",
    "                # teraz key1 orz key2 to numery badanych projektów\n",
    "                \n",
    "                # 1 - odległość, bo oceniamy podobieństwo, a nie dystans\n",
    "                ans.append(1 - df.loc[key1, key2])\n",
    "                \n",
    "    return np.mean(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eba719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Correlation(path, method: list = ['Pearson', 'Spearman', 'Kendall'], significance: bool = True, prob: float = 0.01):\n",
    "    \n",
    "    # print('Plik:', path)\n",
    "    \n",
    "    d = Clustering_from_file(path, variant = 'dataframe')[0]\n",
    "    l = Clustering_from_file(path, variant = 'dataframe')[1]\n",
    "    \n",
    "    dict_df = {}\n",
    "    count_pears = 0\n",
    "    count_spear = 0\n",
    "    count_kend = 0\n",
    "    \n",
    "    for column in list(d.columns):\n",
    "        \n",
    "        dict_df[column] = {}\n",
    "        \n",
    "        # usuwamy 'nan' z obu list\n",
    "        \n",
    "        x1 = []\n",
    "        y1 = []\n",
    "        \n",
    "        for i in range(len(d[column].tolist())):\n",
    "            if str(d[column].tolist()[i]) != 'nan' and str(l[column].tolist()[i]) != 'nan':\n",
    "                x1.append(d[column].tolist()[i])\n",
    "                y1.append(l[column].tolist()[i])\n",
    "            \n",
    "        if len(x1) < 2 or len(y1) < 2 or len(x1) != len(y1):\n",
    "            #print(f\"Wektory projektu {column} nie są równej długości lub wymiar jednego z nich jest mniejszy od 2.\")\n",
    "            continue\n",
    "        # wszystkie 'nan' zostały usunięte\n",
    "                       \n",
    "        if 'Pearson' in method:\n",
    "            \n",
    "            dict_df[column]['Pearson'] = [round(scp.stats.pearsonr(x1, y1)[0], 4)]\n",
    "            \n",
    "            p = scp.stats.pearsonr(x1, y1)[1]\n",
    "\n",
    "            if p<prob:\n",
    "                count_pears += 1\n",
    "                    \n",
    "            if significance == True:\n",
    "                if p<0.05 and p>0.01:\n",
    "                    dict_df[column]['Pearson'].append('(*)')\n",
    "                elif p<0.01 and p>0.001:\n",
    "                    dict_df[column]['Pearson'].append('(**)')\n",
    "                elif p<0.001:\n",
    "                    dict_df[column]['Pearson'].append('(***)')\n",
    "                elif p>0.05:\n",
    "                    dict_df[column]['Pearson'].append('')\n",
    "            dict_df[column]['Pearson'] = ' '.join(map(str, dict_df[column]['Pearson']))\n",
    "            \n",
    "        if 'Spearman' in method:\n",
    "            #print('Spearman: ', scp.stats.spearmanr(d[column], l[column])[0],\n",
    "            #  ', p-value: ', scp.stats.spearmanr(d[column], l[column])[1])\n",
    "            \n",
    "            dict_df[column]['Spearman'] = [round(scp.stats.spearmanr(x1, y1)[0], 4)]\n",
    "            \n",
    "            p = scp.stats.spearmanr(x1, y1)[1]\n",
    "            if p<prob:\n",
    "                count_spear += 1\n",
    "                \n",
    "            if significance == True:\n",
    "                if p<0.05 and p>0.01:\n",
    "                    dict_df[column]['Spearman'].append('(*)')\n",
    "                elif p<0.01 and p>0.001:\n",
    "                    dict_df[column]['Spearman'].append('(**)')\n",
    "                elif p<0.001:\n",
    "                    dict_df[column]['Spearman'].append('(***)')\n",
    "                elif p>0.05:\n",
    "                    dict_df[column]['Spearman'].append('')\n",
    "            dict_df[column]['Spearman'] = ' '.join(map(str, dict_df[column]['Spearman']))\n",
    "            \n",
    "        if 'Kendall' in method:\n",
    "            #print('Kendall: ', scp.stats.kendalltau(d[column], l[column])[0],\n",
    "            #  ', p-value: ', scp.stats.kendalltau(d[column], l[column])[1])\n",
    "            dict_df[column]['Kendall'] = [round(scp.stats.kendalltau(x1, y1)[0], 4)]\n",
    "            \n",
    "            p = scp.stats.kendalltau(x1, y1)[1]\n",
    "            if p<prob:\n",
    "                count_kend += 1\n",
    "                \n",
    "            if significance == True:\n",
    "                if p<0.05 and p>0.01:\n",
    "                    dict_df[column]['Kendall'].append('(*)')\n",
    "                elif p<0.01 and p>0.001:\n",
    "                    dict_df[column]['Kendall'].append('(**)')\n",
    "                elif p<0.001:\n",
    "                    dict_df[column]['Kendall'].append('(***)')\n",
    "                elif p>0.05:\n",
    "                    dict_df[column]['Kendall'].append('')\n",
    "            dict_df[column]['Kendall'] = ' '.join(map(str, dict_df[column]['Kendall']))\n",
    "        \n",
    "    df = pd.DataFrame.from_dict(dict_df, orient = 'index', columns = method)\n",
    "    \n",
    "    return df, [count_pears, count_spear, count_kend]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dcfff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Correlation_summary(paths_list: list):\n",
    "    summary_dict = {}\n",
    "    for path in paths_list:\n",
    "        \n",
    "        with open(path, 'r', newline='', encoding=\"utf-8\") as csvfile:\n",
    "            meta = {}\n",
    "            projects = {}\n",
    "            votes = {}\n",
    "            section = \"\"\n",
    "            header = []\n",
    "            reader = csv.reader(csvfile, delimiter=';')\n",
    "            for row in reader:\n",
    "                if str(row[0]).strip().lower() in [\"meta\", \"projects\", \"votes\"]:\n",
    "                    section = str(row[0]).strip().lower()\n",
    "                    header = next(reader)\n",
    "                elif section == \"meta\":\n",
    "                    meta[row[0]] = row[1].strip()\n",
    "                elif section == \"projects\":\n",
    "                    projects[row[0]] = {}\n",
    "                    for it, key in enumerate(header[1:]):\n",
    "                        projects[row[0]][key.strip()] = row[it+1].strip()\n",
    "                elif section == \"votes\":\n",
    "                    votes[row[0]] = {}\n",
    "                    for it, key in enumerate(header[1:]):\n",
    "                        votes[row[0]][key.strip()] = row[it+1].strip()\n",
    "                    \n",
    "        if 'subunit' in list(meta.keys()):\n",
    "            desc = meta['country'] + ', ' + meta['unit'] + ', ' + meta['subunit'] + ' ' + meta['instance']\n",
    "        else:\n",
    "            desc = meta['country'] + ', ' + meta['unit'] + ', ' + meta['instance']\n",
    "        \n",
    "        cor = Correlation(path, significance = False)\n",
    "        \n",
    "        summary_dict[desc] = {'Number of projects': len(projects.keys()),\n",
    "                              'Pearson': np.mean([float(i) for i in cor[0]['Pearson'].tolist()]),\n",
    "                              '# of Pearson *** signif. coefs.': cor[1][0],\n",
    "                              'Spearman': np.mean([float(i) for i in cor[0]['Spearman'].tolist()]),\n",
    "                              '# of Spearman *** signif. coefs.': cor[1][1],\n",
    "                              'Kendall': np.mean([float(i) for i in cor[0]['Kendall'].tolist()]),\n",
    "                              '# of Kendall *** signif. coefs.': cor[1][2]}\n",
    "        \n",
    "    return pd.DataFrame.from_dict(summary_dict, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72504e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_table(path, variant): \n",
    "    \n",
    "    if variant not in ['ling', 'jacc']:\n",
    "        raise ValueError(\"Argument 'variant' must be one of: 'ling', 'jacc'.\")\n",
    "        \n",
    "    if variant == 'ling':\n",
    "        df = Clustering_from_file(path, variant = 'dataframe')[1]\n",
    "    elif variant == 'jacc':\n",
    "        df = Clustering_from_file(path, variant = 'dataframe')[0]\n",
    "    \n",
    "    count_not_reject = 0\n",
    "    count10 = 0\n",
    "    count5 = 0\n",
    "    count1 = 0\n",
    "    count01 = 0\n",
    "    \n",
    "    for i, column in enumerate(list(df.columns)):\n",
    "        vec = df[column].tolist()\n",
    "        x = vec[:i] + vec[i+1:]\n",
    "        \n",
    "        if len(x) < 1:\n",
    "            continue\n",
    "        \n",
    "        jarquebera = scp.stats.jarque_bera(x)\n",
    "        p = jarquebera.pvalue\n",
    "        if p <= 0.001:\n",
    "            count01 += 1\n",
    "        if p > 0.001 and p <= 0.01:\n",
    "            count1 += 1\n",
    "        if p > 0.01 and p <= 0.05:\n",
    "            count5 += 1\n",
    "        if p > 0.05 and p <= 0.1:\n",
    "            count10 += 1\n",
    "        if p > 0.1:\n",
    "            count_not_reject += 1\n",
    "            \n",
    "    dict_for_table = {'Liczba projektów': len(list(df.columns)),\n",
    "                      'p > 0.1': count_not_reject,\n",
    "                      '0.05 < p <= 0.1': count10,\n",
    "                      '0.01 < p <= 0.05': count5,\n",
    "                      '0.001 < p <= 0.01': count1,\n",
    "                      'p <= 0.001': count01}\n",
    "    \n",
    "    dataframe = pd.DataFrame.from_dict(dict_for_table, orient = 'index')\n",
    "    dataframe.columns = [Clustering_from_file(path, variant = 'desc')]\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39219255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hamming(A, B) -> int:\n",
    "    return len(A - B) + len(B - A)\n",
    "\n",
    "def Jaccard(A, B) -> float:\n",
    "    if len(A.union(B)) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (len(A - B) + len(B - A))/len(A.union(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e3d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clustering_from_file(path, \n",
    "                         method = 'kmeans', \n",
    "                         N: int = 12, \n",
    "                         eps: float = 0.95, \n",
    "                         min_samples: int = 3, \n",
    "                         bandwidth: float = None, \n",
    "                         min_bin_freq: float = 1, \n",
    "                         show: bool = False,\n",
    "                         variant = 'jacc'):\n",
    "    \n",
    "    ##############################################################################\n",
    "    #                                    OPIS                                    #\n",
    "    #path -> ścieżka do pliku z danymi .pb\n",
    "    #N -> liczba klastrów (tylko do kmeans i spectral)\n",
    "    #eps, min_samples -> tylko do dbscan\n",
    "    #bandwidth -> tylko do meanshift\n",
    "    #show -> jeśli 'True', wyświetla header z pliku pabulib, czyli info o wyborach\n",
    "    ##############################################################################\n",
    "    \n",
    "    if variant not in ['jacc', 'ling', 'dataframe', 'desc']:\n",
    "        raise ValueError(\"Argument 'variant' must be one of: 'jacc', 'ling', 'dataframe', 'desc'.\")\n",
    "        \n",
    "    if method not in ['kmeans', 'dbscan', 'meanshift', 'spectral']:\n",
    "        raise ValueError(\"Argument 'method' must be one of: 'kmeans', 'dbscan', 'meanshift', 'spectral'.\")\n",
    "    \n",
    "    #Krok 1. Odczytujemy plik z danymi o wyborach\n",
    "\n",
    "    with open(path, 'r', newline='', encoding=\"utf-8\") as csvfile:\n",
    "        meta = {}\n",
    "        projects = {}\n",
    "        votes = {}\n",
    "        section = \"\"\n",
    "        header = []\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        for row in reader:\n",
    "            if str(row[0]).strip().lower() in [\"meta\", \"projects\", \"votes\"]:\n",
    "                section = str(row[0]).strip().lower()\n",
    "                header = next(reader)\n",
    "            elif section == \"meta\":\n",
    "                meta[row[0]] = row[1].strip()\n",
    "            elif section == \"projects\":\n",
    "                projects[row[0]] = {}\n",
    "                for it, key in enumerate(header[1:]):\n",
    "                    projects[row[0]][key.strip()] = row[it+1].strip()\n",
    "            elif section == \"votes\":\n",
    "                votes[row[0]] = {}\n",
    "                for it, key in enumerate(header[1:]):\n",
    "                    votes[row[0]][key.strip()] = row[it+1].strip()\n",
    "\n",
    "    if variant == 'desc':\n",
    "        if 'subunit' in list(meta.keys()):\n",
    "            desc = meta['country'] + ', ' + meta['unit'] + ', ' + meta['subunit'] + ' ' + meta['instance']\n",
    "        else:\n",
    "            desc = meta['country'] + ', ' + meta['unit'] + ', ' + meta['instance']\n",
    "        return desc\n",
    "    \n",
    "    #Krok 2. Przygotowujemy dane\n",
    "    \n",
    "    projects_approval = {}\n",
    "    for p in list(projects.keys()):\n",
    "        S = []\n",
    "        for k in list(votes.keys()):\n",
    "            if p in set(votes[k]['vote'].split(\",\")):\n",
    "                S.append(k)\n",
    "        projects_approval[p] = S\n",
    "\n",
    "    #tworzymy tablicę odległości między projektami\n",
    "    n = len(list(projects.keys()))\n",
    "    Dist_Array = np.zeros((n, n))\n",
    "    path_csv = path\n",
    "    Dist_Array_ling = pd.read_csv(path_csv.replace('.pb', '.csv'))\n",
    "    Dist_Array_ling.index = Dist_Array_ling.columns\n",
    "\n",
    "    for it1, k in enumerate(list(projects.keys())):\n",
    "        for it2, l in enumerate(list(projects.keys())):\n",
    "            if k == l:\n",
    "                continue\n",
    "            else:\n",
    "                Dist_Array[it1, it2] = Jaccard(set(projects_approval[k]), set(projects_approval[l]))\n",
    "\n",
    "    #tworzymy ramkę danych z tablicy odległości\n",
    "    df = pd.DataFrame(data=Dist_Array, index=list(projects.keys()), columns=list(projects.keys()))\n",
    "    df_ling = pd.DataFrame(data=Dist_Array_ling, index=list(projects.keys()), columns=list(projects.keys()))            \n",
    "\n",
    "    if variant == 'dataframe':\n",
    "        return df, df_ling\n",
    "        # kod kończy się tutaj na stworzeniu i zwróceniu ramki danych z odległościami\n",
    "        \n",
    "    #Krok 3. Klastrujemy\n",
    "    # wybór metody klastrowania:\n",
    "    if method == 'kmeans':\n",
    "        if variant == 'jacc':\n",
    "            kmeans = KMeans(n_clusters=N).fit(df)\n",
    "            labels = kmeans.labels_\n",
    "        if variant == 'ling':\n",
    "            N_ling = N\n",
    "            kmeans_ling = KMeans(n_clusters=N).fit(df_ling)\n",
    "            labels_ling = kmeans_ling.labels_\n",
    "\n",
    "    elif method == 'spectral':\n",
    "        if variant == 'jacc':\n",
    "            spectral = SpectralClustering(n_clusters = N, eigen_solver = 'amg').fit(df)\n",
    "            labels = spectral.labels_\n",
    "        if variant == 'ling':\n",
    "            N_ling = N\n",
    "            spectral_ling = SpectralClustering(n_clusters = N, eigen_solver = 'amg').fit(df_ling)\n",
    "            labels_ling = spectral_ling.labels_\n",
    "\n",
    "    elif method == 'meanshift':\n",
    "        if variant == 'jacc':\n",
    "            meanshift = MeanShift(bandwidth = bandwidth, min_bin_freq = min_bin_freq).fit(df)\n",
    "            labels = meanshift.labels_\n",
    "            N = len(set(labels))\n",
    "        if variant == 'ling':\n",
    "            meanshift_ling = MeanShift(bandwidth = bandwidth, min_bin_freq = min_bin_freq).fit(df_ling)\n",
    "            labels_ling = meanshift_ling.labels_\n",
    "            N_ling = len(set(labels))\n",
    "            \n",
    "    elif method == 'dbscan':\n",
    "        if variant == 'jacc':\n",
    "            dbscan = DBSCAN(eps = eps, min_samples = min_samples).fit(df)\n",
    "            labels = dbscan.labels_\n",
    "            N = len(set(labels))\n",
    "        if variant == 'ling':\n",
    "            dbscan_ling = DBSCAN(eps = eps, min_samples = min_samples).fit(df_ling)\n",
    "            labels_ling = dbscan_ling.labels_\n",
    "            N_ling = len(set(labels_ling))\n",
    "    ######################################################################################################\n",
    "\n",
    "    # zwracanie wyników klastrowania\n",
    "    if variant == 'jacc':\n",
    "        Clusters_jacc = {}\n",
    "        for k in range(N):\n",
    "            Clusters_jacc[k] = []\n",
    "            #lista na projekty w k-tym klastrze\n",
    "            for it3, project in enumerate(list(projects.keys())):\n",
    "                if list(labels)[it3] == k:\n",
    "                    Clusters_jacc[k].append(projects[project]['name'])\n",
    "        if method == 'dbscan':\n",
    "            for it3, project in enumerate(list(projects.keys())):\n",
    "                if list(labels)[it3] == -1:\n",
    "                    Clusters_jacc[N-1].append(projects[project]['name'])       \n",
    "        return Clusters_jacc\n",
    "    \n",
    "    if variant == 'ling':\n",
    "        Clusters_ling = {}\n",
    "        for k in range(N_ling):\n",
    "            Clusters_ling[k] = []\n",
    "            #lista na projekty w k-tym klastrze\n",
    "            for it3, project in enumerate(list(projects.keys())):\n",
    "                if list(labels_ling)[it3] == k:\n",
    "                    Clusters_ling[k].append(projects[project]['name'])\n",
    "        if method == 'dbscan':\n",
    "            for it3, project in enumerate(list(projects.keys())):\n",
    "                if list(labels)[it3] == -1:\n",
    "                    Clusters_ling[N-1].append(projects[project]['name'])   \n",
    "        return Clusters_ling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dbec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_eps(d: dict):\n",
    "    max_val = max(d.values())\n",
    "    for i, key in enumerate(list(d.keys())):\n",
    "        if d[key] > max_val - 1:\n",
    "            min_eps = list(d.keys())[i]\n",
    "            break\n",
    "    for i, key in enumerate(reversed(list(d.keys()))):\n",
    "        if d[key] > max_val - 1:\n",
    "            max_eps = list(d.keys())[-i-1]\n",
    "            break\n",
    "    return min_eps, max_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31df05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_eps(path, min_eps: float = 0.5, max_eps: float = 1.5, print_iter: bool = False, depth: int = 3):\n",
    "    start = time.time()\n",
    "\n",
    "    for j in range(3):\n",
    "        if print_iter == True:\n",
    "            print('Iteracja nr', j + 1)\n",
    "        d = {}\n",
    "        num = int(((max_eps - min_eps)*10**(j+1))) + 1\n",
    "        for i, eps in enumerate(np.linspace(start = min_eps, stop = max_eps, num = num)):\n",
    "            if print_iter == True and i % 5 == 0:\n",
    "                print('\\t', i + 1, '/', num, ', eps =', eps)\n",
    "            c = Clustering_from_file(path, method = 'dbscan', eps = eps, min_bin_freq = 2, variant = 'jacc')\n",
    "            d[eps] = len([len(cluster) for cluster in list(c.values())])\n",
    "\n",
    "        min_eps = cut_eps(d)[0]\n",
    "        max_eps = cut_eps(d)[1]\n",
    "\n",
    "    stop = time.time()\n",
    "    \n",
    "    return d, max(d, key=d.get), round((stop - start)/60, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7607f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_random_clustering(projects, numbers = [], max_in_cluster = 8):\n",
    "    \n",
    "    count = 0 \n",
    "    random_projects = [projects[key]['name'] for key in list(projects.keys())]\n",
    "    rnd.shuffle(random_projects)\n",
    "\n",
    "    random_clusters = {}\n",
    "    \n",
    "    if numbers == []:\n",
    "        \n",
    "        # numery klastrów\n",
    "        k = 0\n",
    "        \n",
    "        while len(random_projects[count:]) >= max_in_cluster:\n",
    "\n",
    "            # liczba projektów w obecnym klastrze\n",
    "            r = rnd.randint(2, max_in_cluster)\n",
    "\n",
    "            random_clusters[k] = random_projects[count:count+r]\n",
    "            k += 1\n",
    "\n",
    "            # liczba wykorzystanych dotąd projektów\n",
    "            count = count + r\n",
    "\n",
    "        random_clusters[k] = random_projects[count:]\n",
    "        \n",
    "    else:\n",
    "        for k, r in enumerate(numbers):\n",
    "            random_clusters[k] = random_projects[count:count+r]\n",
    "            count = count + r\n",
    "    \n",
    "    return random_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938f180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_assessment(path, \n",
    "                     n_iter: int = 20, \n",
    "                     n_random: int = 10, \n",
    "                     n_clusters: list = [5, 7, 9, 11, 13, 15, 17], \n",
    "                     stat = 'mean',\n",
    "                     print_iter: bool = False):\n",
    "    \n",
    "    if stat not in ['mean', 'std dev', 'min', 'max']:\n",
    "        raise ValueError(\"Parameter 'stat' must be one of: 'mean', 'std dev', 'min', 'max'\")\n",
    "    \n",
    "    with open(path, 'r', newline='', encoding=\"utf-8\") as csvfile:\n",
    "        meta = {}\n",
    "        projects = {}\n",
    "        votes = {}\n",
    "        section = \"\"\n",
    "        header = []\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        for row in reader:\n",
    "            if str(row[0]).strip().lower() in [\"meta\", \"projects\", \"votes\"]:\n",
    "                section = str(row[0]).strip().lower()\n",
    "                header = next(reader)\n",
    "            elif section == \"meta\":\n",
    "                meta[row[0]] = row[1].strip()\n",
    "            elif section == \"projects\":\n",
    "                projects[row[0]] = {}\n",
    "                for it, key in enumerate(header[1:]):\n",
    "                    projects[row[0]][key.strip()] = row[it+1].strip()\n",
    "            elif section == \"votes\":\n",
    "                votes[row[0]] = {}\n",
    "                for it, key in enumerate(header[1:]):\n",
    "                    votes[row[0]][key.strip()] = row[it+1].strip()\n",
    "    \n",
    "    \n",
    "    summary = {'ling': [], 'kmeans': [], 'spectral': [], 'dbscan': [], 'random': []}\n",
    "\n",
    "    final_summary = {}\n",
    "\n",
    "    if print_iter == True:\n",
    "        print('Klastrowanie algorytmem dbscan')\n",
    "        t1 = time.time()\n",
    "        \n",
    "    dbscan = Clustering_from_file(path, method = 'dbscan', eps = choose_eps(path, print_iter = print_iter)[1], min_bin_freq = 2, show = False, variant = 'jacc')\n",
    "    summary['dbscan'].append(assess_clustering(dbscan, stat = stat, path = path))\n",
    "    n = len(dbscan.keys())\n",
    "    \n",
    "    if print_iter == True:\n",
    "        print('Czas klastrowania dbscan:', round((time.time() - t1)/60, 2), 'minuty. Liczba klastrów:', n)\n",
    "    \n",
    "    \n",
    "    for n_k, k in enumerate(n_clusters):\n",
    "        \n",
    "        if print_iter == True:\n",
    "            print('Liczba klastrów:', k, ', zostało:', len(n_clusters[n_k:])-1)\n",
    "            \n",
    "        if k >= len(projects.keys()):\n",
    "            if print_iter == True:\n",
    "                print('Liczba projektów:', len(projects.keys()),\n",
    "                     '\\nNie można podzielić na', k, 'klastrów.')\n",
    "            break\n",
    "        \n",
    "        for i in range(n_iter):\n",
    "\n",
    "            if print_iter == True:\n",
    "                print('\\tIteracja', i+1, '/', n_iter)\n",
    "\n",
    "            ling = Clustering_from_file(path, method = 'kmeans', N = k, show = False, variant = 'ling')\n",
    "            summary['ling'].append(assess_clustering(ling, stat = stat, path = path))\n",
    "            \n",
    "            kmeans = Clustering_from_file(path, method = 'kmeans', N = k, show = False, variant = 'jacc')\n",
    "            summary['kmeans'].append(assess_clustering(kmeans, stat = stat, path = path))\n",
    "\n",
    "            spectral = Clustering_from_file(path, method = 'spectral', N = k, show = False, variant = 'jacc')\n",
    "            summary['spectral'].append(assess_clustering(spectral, stat = stat, path = path))\n",
    "            \n",
    "\n",
    "            nb = assess_clustering(kmeans, stat = 'numbers', path = path)\n",
    "            \n",
    "            \n",
    "            # wykonujemy n_random losowych klastrowań na każde klastrowanie kmeans\n",
    "            # jako wynik klastrowania losowego zapisujemy średnią z wyników\n",
    "            m = []\n",
    "            for j in range(n_random):\n",
    "                c_random = perform_random_clustering(projects, numbers = nb)\n",
    "                m.append(assess_clustering(c_random, stat = stat, path = path))\n",
    "\n",
    "            summary['random'].append(np.mean(m))\n",
    "            \n",
    "\n",
    "        final_summary[k] = {'linguistic': round(np.mean(summary['ling']), 5),\n",
    "                            'kmeans': round(np.mean(summary['kmeans']), 5),\n",
    "                            'spectral': round(np.mean(summary['spectral']), 5),\n",
    "                            'dbscan': 'NA',\n",
    "                            'random': round(np.mean(summary['random']), 5)}\n",
    "        \n",
    "    if n in list(final_summary.keys()):\n",
    "        final_summary[n]['dbscan'] = round(summary['dbscan'][0], 5)\n",
    "    else:\n",
    "        final_summary[n] = {'linguistic': 'NA',\n",
    "                            'kmeans': 'NA',\n",
    "                            'spectral': 'NA',\n",
    "                            'dbscan': 0,\n",
    "                            'random': 'NA'}\n",
    "        \n",
    "        nb1 = assess_clustering(dbscan, stat = 'numbers', path = path)  \n",
    "        m1 = []\n",
    "        for j in range(n_iter*n_random):\n",
    "            c_random = perform_random_clustering(projects, numbers = nb1)\n",
    "            m1.append(assess_clustering(c_random, stat = stat, path = path))\n",
    "                \n",
    "        final_summary[n]['dbscan'] = round(summary['dbscan'][0], 5)\n",
    "        final_summary[n]['random'] = round(np.mean(m1), 5)\n",
    "        \n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c9265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ba_summary(paths_list: list, \n",
    "               n_iter: int = 10, \n",
    "               n_random: int = 20, \n",
    "               n_clusters: list = [5, 10, 15],\n",
    "               method = 'dict',\n",
    "               stat = 'mean',\n",
    "               print_desc: bool = False,\n",
    "               print_iter: bool = False):\n",
    "\n",
    "    print('Początek obliczeń o godzinie', time.strftime(\"%H:%M:%S\", time.localtime()), '\\n')\n",
    "    before_loop = time.time()\n",
    "    \n",
    "    if method not in ['dict', 'dataframe']:\n",
    "        raise ValueError(\"Argument 'method' must me one of 'dict', 'dataframe'.\")\n",
    "    \n",
    "    final_dict = {}\n",
    "    stopwatch = {}\n",
    "    \n",
    "    left = len(paths_list)\n",
    "    for path in paths_list:\n",
    "        left -= 1\n",
    "        start = time.time()\n",
    "\n",
    "        with open(path, 'r', newline='', encoding=\"utf-8\") as csvfile:\n",
    "            meta = {}\n",
    "            projects = {}\n",
    "            section = \"\"\n",
    "            header = []\n",
    "            reader = csv.reader(csvfile, delimiter=';')\n",
    "            for row in reader:\n",
    "                if str(row[0]).strip().lower() in [\"meta\", \"projects\", \"votes\"]:\n",
    "                    section = str(row[0]).strip().lower()\n",
    "                    header = next(reader)\n",
    "                elif section == \"meta\":\n",
    "                    meta[row[0]] = row[1].strip()\n",
    "                elif section == \"projects\":\n",
    "                    projects[row[0]] = {}\n",
    "                    for it, key in enumerate(header[1:]):\n",
    "                        projects[row[0]][key.strip()] = row[it+1].strip()\n",
    "                        \n",
    "        after_file = time.time()\n",
    "            \n",
    "        if 'subunit' in list(meta.keys()):\n",
    "            desc = meta['country'] + ', ' + meta['unit'] + ', ' + meta['subunit'] + ' ' + meta['instance']\n",
    "        else:\n",
    "            desc = meta['country'] + ', ' + meta['unit'] + ', ' + meta['instance']            \n",
    "\n",
    "        if print_desc == True:\n",
    "            print('Wybory:', desc,\n",
    "                  '\\n\\tOtwieranie pliku zajęło', \n",
    "                  round((after_file - start)*1000, 2), \n",
    "                  'milisekundy.',\n",
    "                 '\\n\\tPozostało plików:', left)\n",
    "        \n",
    "        tab_jacc = normal_table(path, variant = 'jacc')\n",
    "        tab_ling = normal_table(path, variant = 'ling')\n",
    "            \n",
    "        y = {}\n",
    "        y['Liczba projektów'] = {'info': len(list(projects.keys())), \n",
    "                                 'linguistic': '', 'kmeans': '', 'spectral': '', 'dbscan': '', 'random': ''}\n",
    "\n",
    "        #y['% braku odrzuceń H0 [Jacc]'] = {'info': np.asmatrix(tab_jacc.iloc[[1], [0]])[0, 0]/np.asmatrix(tab_jacc.iloc[[0], [0]])[0, 0],\n",
    "        #                                    'linguistic': '', 'kmeans': '', 'spectral': '', 'dbscan': '', 'random': ''}\n",
    "        #y['% braku odrzuceń H0 [ling]'] = {'info': np.asmatrix(tab_ling.iloc[[1], [0]])[0, 0]/np.asmatrix(tab_ling.iloc[[0], [0]])[0, 0],\n",
    "        #                                    'linguistic': '', 'kmeans': '', 'spectral': '', 'dbscan': '', 'random': ''}\n",
    "        \n",
    "        x = final_assessment(path, \n",
    "                n_iter = n_iter, \n",
    "                n_random = n_iter, \n",
    "                n_clusters = n_clusters,\n",
    "                stat = stat,\n",
    "                print_iter = print_iter)\n",
    "        \n",
    "        keys_list = list(x.keys())\n",
    "        keys_list.sort()\n",
    "        x = {key: x[key] for key in keys_list}\n",
    "        \n",
    "        for key in list(x.keys()):\n",
    "            x[key]['info'] = ''\n",
    "        \n",
    "        y.update(x)\n",
    "        final_dict[desc] = y\n",
    "\n",
    "        now = time.time()\n",
    "        \n",
    "        if print_desc == True:\n",
    "            print('\\tOcena klastrowania zajęła', \n",
    "                  round((now - after_file)/60, 2),\n",
    "                  'minuty.\\n\\tOd początku minęło', \n",
    "                  round((now - before_loop)/60, 2), \n",
    "                  'minuty.\\n')\n",
    "            \n",
    "        stopwatch[desc] = {'number of projects': len(projects.keys()), 'time': round((now - after_file)/60, 2)}\n",
    "        \n",
    "    if method == 'dict':\n",
    "        return final_dict, stopwatch\n",
    "    \n",
    "    elif method == 'dataframe':\n",
    "        return pd.DataFrame.from_dict({(i,j): final_dict[i][j] \n",
    "                                       for i in final_dict.keys() \n",
    "                                       for j in final_dict[i].keys()}, \n",
    "                                       orient = 'index'), pd.DataFrame.from_dict(stopwatch, orient = 'index')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
